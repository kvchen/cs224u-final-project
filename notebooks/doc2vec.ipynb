{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffled = list(self.sentences)\n",
    "        random.shuffle(shuffled)\n",
    "        return shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {\n",
    "    '../datasets/train.c': 'TRAIN_CONTROVERSIAL',\n",
    "    '../datasets/train.nc': 'TRAIN_UNCONTROVERSIAL',\n",
    "    '../datasets/test.c': 'TEST_CONTROVERSIAL',\n",
    "    '../datasets/test.nc': 'TEST_UNCONTROVERSIAL',\n",
    "}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kvchen/code/cs224u-final-project/notebooks/.env/lib/python3.6/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "/Users/kvchen/code/cs224u-final-project/notebooks/.env/lib/python3.6/site-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=200, sample=1e-4, negative=5, workers=7)\n",
    "\n",
    "model.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(sentences.sentences_perm(), total_examples=len(sentences.sentences), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kvchen/code/cs224u-final-project/notebooks/.env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('he', 0.8239975571632385),\n",
       " ('him', 0.7968652844429016),\n",
       " ('president', 0.791419506072998),\n",
       " ('his', 0.7529799342155457),\n",
       " ('putin', 0.7302655577659607),\n",
       " ('obama', 0.7178451418876648),\n",
       " ('this', 0.7112518548965454),\n",
       " ('that', 0.7025802135467529),\n",
       " ('.', 0.6967028975486755),\n",
       " ('donald', 0.6935111284255981)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('twitter.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 26302\n",
    "num_train_uc = 346313\n",
    "\n",
    "train_arrays = np.zeros((num_train * 2, 200))\n",
    "train_labels = np.zeros(num_train * 2)\n",
    "\n",
    "for i in range(num_train):\n",
    "    prefix_train_c = f\"TRAIN_CONTROVERSIAL_{i}\"\n",
    "    train_arrays[i] = model[prefix_train_c]\n",
    "    train_labels[i] = 1\n",
    "\n",
    "for idx, i in enumerate(random.sample(range(num_train_uc), num_train)):\n",
    "    prefix_train_uc = f\"TRAIN_UNCONTROVERSIAL_{i}\"\n",
    "    ix = idx + num_train\n",
    "    train_arrays[ix] = model[prefix_train_uc]\n",
    "    train_labels[ix] = 0\n",
    "\n",
    "# for i in range(12500):\n",
    "#     prefix_train_pos = f\"TRAIN_CONTROVERSIAL_{i}\"\n",
    "#     prefix_train_neg = f\"TRAIN_UNCONTROVERSIAL_{i}\"\n",
    "#     train_arrays[i] = model[prefix_train_pos]\n",
    "#     train_arrays[12500 + i] = model[prefix_train_neg]\n",
    "#     train_labels[i] = 1\n",
    "#     train_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = 26156\n",
    "num_test_uc = 347449\n",
    "\n",
    "test_arrays = np.zeros((num_test * 2, 200))\n",
    "test_labels = np.zeros(num_test * 2)\n",
    "\n",
    "for i in range(num_test):\n",
    "    prefix_test_c = f\"TEST_CONTROVERSIAL_{i}\"\n",
    "    test_arrays[i] = model[prefix_test_c]\n",
    "    test_labels[i] = 1\n",
    "\n",
    "for idx, i in enumerate(random.sample(range(num_test_uc), num_test)):\n",
    "    prefix_test_uc = f\"TEST_UNCONTROVERSIAL_{i}\"\n",
    "    ix = idx + num_test\n",
    "    test_arrays[ix] = model[prefix_test_uc]\n",
    "    test_labels[ix] = 0\n",
    "\n",
    "# test_arrays = np.zeros((25000, 100))\n",
    "# test_labels = np.zeros(25000)\n",
    "\n",
    "# for i in range(12500):\n",
    "#     prefix_test_pos = 'TEST_CONTROVERSIAL_' + str(i)\n",
    "#     prefix_test_neg = 'TEST_UNCONTROVERSIAL_' + str(i)\n",
    "#     test_arrays[i] = model[prefix_test_pos]\n",
    "#     test_arrays[12500 + i] = model[prefix_test_neg]\n",
    "#     test_labels[i] = 1\n",
    "#     test_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5733483713105979"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "\n",
    "comments = tf.placeholder(\n",
    "    tf.float32,\n",
    "    (None, 200),\n",
    "    name=\"comments\",\n",
    ")\n",
    "labels = tf.placeholder(tf.int32, (None, 1), name=\"labels\")\n",
    "\n",
    "h1 = tf.layers.dense(comments, units=512, activation=tf.nn.relu)\n",
    "h2 = tf.layers.dense(h1, units=512, activation=tf.nn.relu)\n",
    "logits = tf.layers.dense(h2, units=2)\n",
    "\n",
    "logits = tf.Print(logits, [logits])\n",
    "\n",
    "# Loss optimization\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=logits,\n",
    "    labels=labels,\n",
    ")\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "# Check accuracy\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, 1, output_type=tf.int32), labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [05:35<00:00, 298.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.049046412\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "#     logdir = os.path.join('summary', str(int(time.time())))\n",
    "#     writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "#     saver = tf.train.Saver()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in trange(int(1e5)):\n",
    "        batch_idx = np.random.choice(len(train_arrays), 100)\n",
    "        _, loss_val, accuracy_val = sess.run(\n",
    "            [train_op, loss, accuracy],\n",
    "            {\n",
    "                comments: train_arrays[batch_idx],\n",
    "                labels: train_labels[batch_idx][:, None],\n",
    "            }\n",
    "        )\n",
    "        \n",
    "#         if i % 100 == 0:\n",
    "#             print(loss_val, accuracy_val)\n",
    "#         writer.add_summary(summary, i)\n",
    "\n",
    "    test_accuracy = sess.run(accuracy, {\n",
    "        comments: test_arrays,\n",
    "        labels: test_labels[:, None],\n",
    "    })\n",
    "    print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
