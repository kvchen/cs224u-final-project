{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "\n",
    "from collections import namedtuple\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('../datasets/glove.840B.300d.w2vformat.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kvchen/code/cs224u-final-project/notebooks/.env/lib/python3.6/site-packages/pandas/io/json/json.py:521: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  obj = concat(self)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('../datasets/RC_2018-03.filtered', lines=True, chunksize=1e4).read()\n",
    "df = df[(df.body != '[deleted]') & (df.body != '[removed]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_matrix(word2vec):\n",
    "    num_words = len(word2vec.vocab)\n",
    "    embed_matrix = np.random.randn(num_words + 2, word2vec.vector_size)\n",
    "\n",
    "    for i in range(num_words):\n",
    "        word = word2vec.index2word[i]\n",
    "        word_embedding = word2vec[word]\n",
    "        \n",
    "        embed_matrix[i] = word_embedding\n",
    "    \n",
    "    return embed_matrix\n",
    "\n",
    "embed_matrix = get_embed_matrix(word2vec)\n",
    "PAD_ID = len(word2vec.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "df['body_tokens'] = df['body'].apply(lambda comment: tokenizer.tokenize(comment.lower()))\n",
    "df['body_token_ids'] = df['body_tokens'].apply(\n",
    "    lambda tokens: [\n",
    "        word2vec.vocab[token].index\n",
    "        for token in tokens\n",
    "        if token in word2vec  # Note! We are throwing out unknown words here\n",
    "    ]\n",
    ")\n",
    "\n",
    "cdf = df[df.controversiality > 0]\n",
    "ncdf = df[df.controversiality == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52574 controversial examples\n",
      "52574 uncontroversial examples\n"
     ]
    }
   ],
   "source": [
    "# We need to resample the noncontroversial samples because the dataset is highly imbalanced\n",
    "ncdf_resampled = resample(ncdf, replace=False, n_samples=len(cdf), random_state=123456)\n",
    "\n",
    "print(f\"{len(cdf)} controversial examples\")\n",
    "print(f\"{len(ncdf_resampled)} uncontroversial examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([cdf, ncdf_resampled])\n",
    "train_df, test_df = train_test_split(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = namedtuple(\n",
    "    'Batch',\n",
    "    ['comment_tokens', 'comment_token_ids', 'comment_lens', 'comment_controversialities'],\n",
    ")\n",
    "\n",
    "def get_batch(train_df, batch_size=100, max_comment_len=100):\n",
    "    sampled = train_df.sample(batch_size)\n",
    "    \n",
    "    comment_tokens = sampled.body_tokens\n",
    "    \n",
    "    comment_token_ids = sampled.body_token_ids.apply(\n",
    "        lambda token_ids: (token_ids + ([PAD_ID] * max_comment_len))[:max_comment_len]\n",
    "    )\n",
    "    comment_token_ids = np.array([[int(x) for x in c] for c in comment_token_ids])\n",
    "    comment_lens = np.array(sampled.body_token_ids.str.len())\n",
    "    comment_controversialities = np.array(sampled.controversiality > 0)\n",
    "    \n",
    "    return Batch(comment_tokens, comment_token_ids, comment_lens, comment_controversialities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(embed_matrix, max_comment_len=100):\n",
    "    # Placeholders\n",
    "\n",
    "    word_embeddings = tf.placeholder(tf.float32, embed_matrix.shape, name=\"word_embeddings\")\n",
    "    comment_token_ids = tf.placeholder(tf.int32, (None, max_comment_len), name=\"comment_token_ids\")\n",
    "    comment_lens = tf.placeholder(tf.int32, (None,), name=\"comment_lens\")\n",
    "    comment_controversialities = tf.placeholder(tf.int32, (None,), name=\"comment_controversialities\")\n",
    "\n",
    "    # Embedding lookup\n",
    "\n",
    "    with tf.variable_scope('embed'):\n",
    "        comment_vecs = tf.nn.embedding_lookup(word_embeddings, comment_token_ids)\n",
    "\n",
    "    # RNN\n",
    "\n",
    "    lstm_cell_size = 100\n",
    "\n",
    "    with tf.variable_scope('lstm'):\n",
    "        lstm_forward = tf.contrib.rnn.BasicLSTMCell(lstm_cell_size)\n",
    "        lstm_backward = tf.contrib.rnn.BasicLSTMCell(lstm_cell_size)\n",
    "        \n",
    "        (fw_out, bw_out), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "            lstm_forward,\n",
    "            lstm_backward,\n",
    "            comment_vecs,\n",
    "            sequence_length=tf.minimum(comment_lens, max_comment_len),\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "\n",
    "        rnn_out = tf.concat([fw_out, bw_out], axis=2)[:, -1, :]\n",
    "\n",
    "    with tf.variable_scope('output'):\n",
    "        logits = tf.contrib.layers.fully_connected(\n",
    "            rnn_out,\n",
    "            num_outputs=2,\n",
    "            activation_fn=None,\n",
    "        )\n",
    "\n",
    "    # Loss optimization\n",
    "\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits,\n",
    "        labels=comment_controversialities[:, None],\n",
    "    )\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "    # Check accuracy\n",
    "\n",
    "    correct = tf.equal(\n",
    "        tf.argmax(logits, 1, output_type=tf.int32),\n",
    "        comment_controversialities,\n",
    "    )\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    tf.summary.scalar('Loss', loss)\n",
    "    tf.summary.scalar('Accuracy', accuracy)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'train': train_op,\n",
    "        'summary': summary_op,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph_ops = create_graph(embed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "#     logdir = os.path.join('summary', str(int(time.time())))\n",
    "#     writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "#     saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(int(1e4)):\n",
    "        batch = get_batch(train_df)        \n",
    "        _, loss = sess.run(\n",
    "            [graph_ops['train'], graph_ops['loss']],\n",
    "            {\n",
    "                \"word_embeddings:0\": embed_matrix,\n",
    "                \"comment_token_ids:0\": batch.comment_token_ids,\n",
    "                \"comment_lens:0\": batch.comment_lens,\n",
    "                \"comment_controversialities:0\": batch.comment_controversialities,\n",
    "            }\n",
    "        )\n",
    "        print(loss)\n",
    "#         writer.add_summary(summary, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
